Zillow Prize Competition EDA by Hadi Ramezani-Dakhel
========================================================
# About the competition:

Zillow's Home Value Predition competition is a two-round competition in which the ultimate goal is to improve the home predition algotrithm of Zillow (aka Zestimate).

During the first round of the competition, the objective is to predict the Zestimate's residual error, i.e. we need to predit where Zestimate fails and where it succeeds:

$$logerror=log(Zestimate)-log(SalePrice)$$

To make a successful predictive model our algorithm must be as good as Zillows' algorithm (not better and not worse). In the second stage, however, the objective is to actually improve the home value prediction algorithm.

Here, we are provided with information on ~3M properties of three California counties including Los Angles, Orange, and Ventura (properties_2016.csv).Among these ~3M properties, we only have the residual error information of ~90K properties (train_2016_v2.csv). Therefore, our focus for this EDA would be on those properties.   


```{r echo=FALSE, message=FALSE, warning=FALSE, packages}
# The other parameters for "message" and "warning" should also be set to FALSE
# for other code chunks once you have verified that each plot comes out as you
# want it to. This will clean up the flow of your report.

library(ggplot2)
library(dplyr)
library(gridExtra)
library(stringr)
library(GGally)
```

Let's read in the data first.

```{r echo=FALSE, Load_the_Data}
# Load the Data
setwd("H:/udacity/data_analyst/p4_exploratory_data_analysis/ZillowPrize_EDA")
properties_2016 = read.csv("renamed_properties_2016.csv")
train_2016 = read.csv("renamed_train_2016.csv")
```

## Renaming the features
Philipp Spachtholz from kaggle ("https://www.kaggle.com/philippsp/exploratory-analysis-zillow/notebook") has proposed a much better naming for the features. I will use his names (with some updates of mine) to make the feature names self-explanatory.

#```{r echo=FALSE, Rename}
properties_2016 = properties_2016 %>% rename(
  id_parcel = parcelid,
  build_year = yearbuilt,
  area_basement = basementsqft,
  area_patio = yardbuildingsqft17,
  area_shed = yardbuildingsqft26, 
  area_pool = poolsizesum,  
  area_lot = lotsizesquarefeet, 
  area_garage = garagetotalsqft,
  area_firstfloor_finished = finishedfloor1squarefeet,
  area_total_calc = calculatedfinishedsquarefeet,
  area_base = finishedsquarefeet6,
  area_live_finished = finishedsquarefeet12,
  area_liveperi_finished = finishedsquarefeet13,
  area_total_finished = finishedsquarefeet15,  
  area_unknown = finishedsquarefeet50,
  num_unit = unitcnt, 
  num_story = numberofstories,  
  num_room = roomcnt,
  num_bathroom = bathroomcnt,
  num_bedroom = bedroomcnt,
  num_bathroom_calc = calculatedbathnbr,
  num_bath = fullbathcnt,  
  num_75_bath = threequarterbathnbr, 
  num_fireplace = fireplacecnt,
  num_pool = poolcnt,  
  num_garage = garagecarcnt,  
  region_county = regionidcounty,
  region_city = regionidcity,
  region_zip = regionidzip,
  region_neighbor = regionidneighborhood,  
  tax_total = taxvaluedollarcnt,
  tax_building = structuretaxvaluedollarcnt,
  tax_land = landtaxvaluedollarcnt,
  tax_property = taxamount,
  tax_year = assessmentyear,
  tax_delinquency = taxdelinquencyflag,
  tax_delinquency_year = taxdelinquencyyear,
  zoning_property = propertyzoningdesc,
  type_zoning_landuse = propertylandusetypeid,
  zoning_landuse_county = propertycountylandusecode,
  flag_fireplace = fireplaceflag, 
  flag_tub = hashottuborspa,
  type_quality = buildingqualitytypeid,
  type_framing = buildingclasstypeid,
  type_material = typeconstructiontypeid,
  type_deck = decktypeid,
  type_story = storytypeid,
  type_heating = heatingorsystemtypeid,
  type_aircon = airconditioningtypeid,
  type_architectural= architecturalstyletypeid
)
rename(train_2016)
train_2016 = train_2016 %>% rename(
  id_parcel = parcelid,
  date = transactiondate
)
properties_2016 = properties_2016 %>% 
  mutate(tax_delinquency = ifelse(tax_delinquency=="Y",1,0),
         flag_fireplace = ifelse(flag_fireplace=="Y",1,0),
         flag_tub = ifelse(flag_tub=="Y",1,0))
#```

Now, let's write the data into new files.

#```{r write_newfiles}
write.csv(properties_2016, "renamed_properties_2016.csv")
write.csv(train_2016, "renamed_train_2016.csv")
#```

Let's take a brief look at the datasets. The variables, their type, etc.

```{r echo=FALSE, Summary_of_Data}
str(properties_2016)
str(train_2016)
names(properties_2016)
```

The dataset "properties_2016" consists of 58 variables and 2,985,217 observations. The dataset "train_2016_v2.csv" contains 3 variables and 90275 observations. 

Let's merge these two datasets, and create a single dataset that contains both the property information and the transaction information.

```{r echo=FALSE, merge_datasets}
zillow = merge(train_2016, properties_2016, by = "id_parcel")
str(zillow)
head(zillow)

```

This now will be the main dataset that we'll be working with.


# Univariate Plots Section

Our objective is to predict the residual error. So, let's plot a histogram of logerror. I'll plot all histograms in terms of percentages to make a better sense out of them. I'll first do a summary to help me setup the graph.

```{r echo=FALSE, Ditribution_of_logerror }
summary(zillow$logerror)
ggplot(aes(x = logerror, y = ..count../sum(..count..)*100), data = zillow) +
  geom_histogram(fill = 'blue', binwidth = 0.05) +
  xlab("logerror") + ylab("%") +
  coord_cartesian(x=c(-1, 1), y=c(0,45))
```

The x-axis here is the logerror. It actually shows the order of magnitude differences between the estimated values and the sale price. For instance, logerror= 1 indicates that Zestimate overestimated a property value by an order of magnitude, e.g. the sale price was \$100,000 but Zestimate predicted \$1000,000! Let's get a summary of logerror:

```{r echo=FALSE, summary_logerror}
summary(zillow$logerror)
```

Mean and medians are both positive. This indicates that Zestimate on average overestimates the house prices. 

Let's split this over three different counties to see how the distribution looks like.

```{r echo=FALSE, Ditrib_logerror_by_county }

names(table(zillow$region_county))

p1 = ggplot(aes(x = logerror, y = ..count../sum(..count..)*100), data = subset(zillow, region_county == 1286)) +
  geom_histogram(fill = 'blue', binwidth = 0.05) +
  xlab("logerror") + ylab("%") +
  labs(title = "1286") +
  coord_cartesian(x=c(-1, 1), y=c(0,45))

p2 = ggplot(aes(x = logerror, y = ..count../sum(..count..)*100), data = subset(zillow, region_county == 2061)) +
  geom_histogram(fill = 'black', binwidth = 0.05) +
  xlab("logerror") + ylab("%") +
  labs(title = "2061") +
  coord_cartesian(x=c(-1, 1), y=c(0,45))

p3 = ggplot(aes(x = logerror, y = ..count../sum(..count..)*100), data = subset(zillow, region_county == 3101)) +
  geom_histogram(fill = 'red', binwidth = 0.05) +
  xlab("logerror") + ylab("%") +
  labs(title = "3101") +
  coord_cartesian(x=c(-1, 1), y=c(0,45))

grid.arrange(p1, p2, p3, ncol = 3)

```

While the distributions look similar, the code "3101" has longer tails. This could simply be because of higher number of transactions in that county.

I would like to get a overall idea of the size and the age of the buildings.

```{r echo=FALSE, size_hitogram}
p1 = ggplot(aes(x = area_total_calc, y = ..count../sum(..count..)*100), data = subset(zillow, !is.na(area_total_calc))) +
  geom_histogram(fill = "orange", binwidth = 100) +
  xlab("total area") + ylab("%") +
  coord_cartesian(x=c(500, 7500), y=c(0,8)) +
  labs(title = "size linear scale")

p2 = ggplot(aes(x = area_total_calc, y = ..count../sum(..count..)*100), data = subset(zillow, !is.na(area_total_calc))) +
  geom_histogram(fill = "green", binwidth = 0.03) +
  xlab("total") + ylab("%") +
  coord_cartesian(x=c(500, 7500), y=c(0,8)) +
  scale_x_log10() +
  labs(title = "size log scale")
grid.arrange(p1, p2, ncol = 2)
```

By scaling the x axis to log10, the histogram looks like a normal distribution. 

Let's get a summary of the data as well.

```{r echo=FALSE, size_summary}
summary(zillow$area_total_calc)
```

The average size 1773 sqft and the max size is 2273 sqft. These values make sense. However, min size is 2! This does not make sense. We should be careful with this when we try to fit a model to the data. 

Next, I'll create a new feature called "age". This feature contains the age of each building. I'll then plot a histogram of age.

```{r echo=FALSE, age_hitogram}
zillow$age_year = 2017 - zillow$build_year

ggplot(aes(x = age_year, y = ..count../sum(..count..)*100), data = subset(zillow, !is.na(build_year))) +
  geom_histogram(fill = "lightcoral", binwidth = 5) +
  xlab("age") + ylab("%") +
  coord_cartesian(x=c(0, 120), y=c(0,10)) +
  labs(title = "age") 
```


We don't have too many new buidings in the dataset. Let's also get a summary of the data.

```{r echo=FALSE, age_summary}
summary(zillow$age_year)
```

The mean age of the building is ~48 years. The mean and medians are very close here.

# Univariate Analysis

### What is the structure of your dataset?

The main dataset contains transactions/logerror data for 90275 properties with 58 features. The features include information such as size, tax, number of rooms, built year as well as x,y coordinates and zone information.  

### What is/are the main feature(s) of interest in your dataset?

The main feature of interest is the logerror which shows the order of magnitude deviation of the predicted price from the sale price.   

### What other features in the dataset do you think will help support your \
investigation into your feature(s) of interest?

I think the coordinate of each property (x and y) and the properties density in a region (this is not included in the dataset) are important features which could be related to the magnitude of logerror. I plan to perform some EDA on the properties coordinates and their correlation with logerro these features. Calculating the feature "density" is quite challanigng and suitable for model building later on.

### Did you create any new variables from existing variables in the dataset?

I created a new feature called "age" (the age of each properties expressed in number of years) to better show if a property is newly built or not. The existing feature "year_built" contains this information but I think it's quite easier to comprehend the age rahter than the year_built.

### Of the features you investigated, were there any unusual distributions? \
Did you perform any operations on the data to tidy, adjust, or change the form \
of the data? If so, why did you do this?

The distribution of the graphs (logerror, size, age) were not unexpected. The distribution of the size histogram was right skewed. I log-transformed that distribution and the transformed graph looked like a normal distribution. 


# Bivariate Plots Section

I would like to look at the correlation between different features. There are now 61 features in the dataset! We either have to select important features by intuition or find another criterion! Let's do both; first, I'll drop features with more than 50% of missing values.

```{r echo=FALSE, drop_features}
zillow_selected = zillow[, -which(colMeans(is.na(zillow)) > 0.5)]
str(zillow_selected)
```

This leaves us with 35 features. We can now select some features and make correlation plots.

First, let's create the correlain graphs for logerror, num_bedroom, num_bathroom, num_room, and num_units. 

```{r echo=FALSE, message=FALSE, warning=FALSE, correlation_for_numbers}
num_features = zillow_selected[, c("logerror", "num_bedroom", "num_bathroom_calc", "num_room", "num_unit")]
ggpairs(num_features)
```

We see that there is a strong correlation between the number of bathrooms and the number of bedroom in a properties which makes sense. 
The correlation between the logerror and the number of bedroom shows the distribution of the logerror is pretty wide for the properties with 3-5 bedrooms.

Now, let's look at the correlation between the features related to the size and also the age of the buidings.

```{r echo=FALSE, message=FALSE, warning=FALSE, correlation_for_areas_year}
area_year_features = zillow_selected[, c("logerror", "area_total_calc", "area_live_finished", "area_lot", "age_year")]
ggpairs(area_year_features)
```

There is a perfect correlation between the "area_total_calc" and "area_live_finished" with correlation coefficient of 1. These two features should be combined together to create a single feature when making a predictive model.

The correlation plots between the "logerror" and "area_total_calc", "area_live_finished", and "area_lot" shows the logerror is smaller for larger properties. Again, we should keep in mind that the logerror tells us about the order of magnitude of error. 


Finally, let's make a correlation plots for the tax related features.

```{r echo=FALSE, message=FALSE, warning=FALSE, correlation_for_areas_year}
tax_features = zillow_selected[, c("logerror", "tax_building", "tax_land", "tax_total")]
ggpairs(tax_features)
```

We see that there is not a strong correlation between the tax features and the logerror. However, there is a strong correlation between the tax properties themselevs which is not surprising.

Let's explore the month in which the transations occured and the correlation with logerror. To do this, I will created a new feature "trans_month" that contains the months informatin (1, 2, 3, etc). The reasoning behind this comes from the fact that "season change" drives the real estate market. I want to see whether Zestimate takes this into account or not, i.e. is there a strong correlation between the transation month and the logerror?

```{r echo=FALSE, trans_month}
zillow$trans_month = format(as.Date(zillow$date), "%m")

ggplot(aes(x = trans_month, y = ..count../sum(..count..)*100), data = subset(zillow, !is.na(trans_month))) +
  geom_bar(fill = "brown") +
  xlab("trans_month") + ylab("%") +
  #coord_cartesian(x=c(0, 120), y=c(0,10)) +
  labs(title = "transaction month") 
```

This histogram only tells us about the transaction data avaiable to us. It contains no information about the total number of transations in each month.

Now, let's plot the avergae logerror vs transaction month.

```{r Conditional Summaries}
trans_month_groups = group_by(zillow, trans_month)
zillow.logerror_by_month = summarise(trans_month_groups,
          logerror_mean = mean(logerror),
          logerror_median = median(logerror),
          n = n())

ggplot(data = zillow.logerror_by_month, aes(x = trans_month, y = logerror_mean)) +
  geom_point(color = "blue", size = 3.5) + geom_line(color = "blue", size = 1.5, group = 1)

```


Very interesting results. The average error decreases as we enter the spring (March, April, etc) and it rises once the summer is finished (September, October, etc). This could very much be a general trend.


# Bivariate Analysis

### Talk about some of the relationships you observed in this part of the \
investigation. How did the feature(s) of interest vary with other features in \
the dataset?

### Did you observe any interesting relationships between the other features \
(not the main feature(s) of interest)?

### What was the strongest relationship you found?


# Multivariate Plots Section

> **Tip**: Now it's time to put everything together. Based on what you found in
the bivariate plots section, create a few multivariate plots to investigate
more complex interactions between variables. Make sure that the plots that you
create here are justified by the plots you explored in the previous section. If
you plan on creating any mathematical models, this is the section where you
will do that.

```{r echo=FALSE, Multivariate_Plots}

```

# Multivariate Analysis

### Talk about some of the relationships you observed in this part of the \
investigation. Were there features that strengthened each other in terms of \
looking at your feature(s) of interest?

### Were there any interesting or surprising interactions between features?

### OPTIONAL: Did you create any models with your dataset? Discuss the \
strengths and limitations of your model.

------

# Final Plots and Summary

> **Tip**: You've done a lot of exploration and have built up an understanding
of the structure of and relationships between the variables in your dataset.
Here, you will select three plots from all of your previous exploration to
present here as a summary of some of your most interesting findings. Make sure
that you have refined your selected plots for good titling, axis labels (with
units), and good aesthetic choices (e.g. color, transparency). After each plot,
make sure you justify why you chose each plot by describing what it shows.

### Plot One
```{r echo=FALSE, Plot_One}

```

### Description One


### Plot Two
```{r echo=FALSE, Plot_Two}

```

### Description Two


### Plot Three
```{r echo=FALSE, Plot_Three}

```

### Description Three

------

# Reflection

> **Tip**: Here's the final step! Reflect on the exploration you performed and
the insights you found. What were some of the struggles that you went through?
What went well? What was surprising? Make sure you include an insight into
future work that could be done with the dataset.

> **Tip**: Don't forget to remove this, and the other **Tip** sections before
saving your final work and knitting the final report!