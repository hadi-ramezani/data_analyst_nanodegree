---
output:
  html_document: default
  pdf_document: default
---
Zillow Prize Competition EDA by Hadi Ramezani-Dakhel
========================================================
# About the competition:

Zillow's Home Value Predition competition is a two-round competition in which the ultimate goal is to improve the home predition algotrithm of Zillow (aka Zestimate).

During the first round of the competition, the objective is to predict the Zestimate's residual error, i.e. we need to predict where Zestimate fails and where it succeeds:

$$logerror=log(Zestimate)-log(SalePrice)$$

To make a successful predictive model our algorithm must be as good as Zillows' algorithm (not better and not worse). In the second stage, however, the objective is to actually improve the home value prediction algorithm.

Here, we are provided with information on ~3M properties of three California counties including Los Angles, Orange, and Ventura (properties_2016.csv).Among these ~3M properties, we only have the residual error information of ~90K properties (train_2016_v2.csv). Therefore, our focus for this EDA would be on those properties.   


```{r echo=FALSE, message=FALSE, warning=FALSE, packages}
# The other parameters for "message" and "warning" should also be set to FALSE
# for other code chunks once you have verified that each plot comes out as you
# want it to. This will clean up the flow of your report.

library(ggplot2)
library(dplyr)
library(gridExtra)
library(stringr)
library(GGally)
library(leaflet)
```

Let's read in the data first.

```{r echo=FALSE, message=FALSE, warning=FALSE, Load_the_Data}
# Load the Data
setwd("H:/udacity/data_analyst/p4_exploratory_data_analysis/ZillowPrize_EDA")
properties_2016 = read.csv("renamed_properties_2016.csv")
train_2016 = read.csv("renamed_train_2016.csv")
```

## Renaming the features
Philipp Spachtholz from kaggle ("https://www.kaggle.com/philippsp/exploratory-analysis-zillow/notebook") has proposed a much better naming for the features. I will use his names (with some updates of mine) to make the feature names self-explanatory.

#```{r echo=FALSE, Rename}
properties_2016 = properties_2016 %>% rename(
  id_parcel = parcelid,
  build_year = yearbuilt,
  area_basement = basementsqft,
  area_patio = yardbuildingsqft17,
  area_shed = yardbuildingsqft26, 
  area_pool = poolsizesum,  
  area_lot = lotsizesquarefeet, 
  area_garage = garagetotalsqft,
  area_firstfloor_finished = finishedfloor1squarefeet,
  area_total_calc = calculatedfinishedsquarefeet,
  area_base = finishedsquarefeet6,
  area_live_finished = finishedsquarefeet12,
  area_liveperi_finished = finishedsquarefeet13,
  area_total_finished = finishedsquarefeet15,  
  area_unknown = finishedsquarefeet50,
  num_unit = unitcnt, 
  num_story = numberofstories,  
  num_room = roomcnt,
  num_bathroom = bathroomcnt,
  num_bedroom = bedroomcnt,
  num_bathroom_calc = calculatedbathnbr,
  num_bath = fullbathcnt,  
  num_75_bath = threequarterbathnbr, 
  num_fireplace = fireplacecnt,
  num_pool = poolcnt,  
  num_garage = garagecarcnt,  
  region_county = regionidcounty,
  region_city = regionidcity,
  region_zip = regionidzip,
  region_neighbor = regionidneighborhood,  
  tax_total = taxvaluedollarcnt,
  tax_building = structuretaxvaluedollarcnt,
  tax_land = landtaxvaluedollarcnt,
  tax_property = taxamount,
  tax_year = assessmentyear,
  tax_delinquency = taxdelinquencyflag,
  tax_delinquency_year = taxdelinquencyyear,
  zoning_property = propertyzoningdesc,
  type_zoning_landuse = propertylandusetypeid,
  zoning_landuse_county = propertycountylandusecode,
  flag_fireplace = fireplaceflag, 
  flag_tub = hashottuborspa,
  type_quality = buildingqualitytypeid,
  type_framing = buildingclasstypeid,
  type_material = typeconstructiontypeid,
  type_deck = decktypeid,
  type_story = storytypeid,
  type_heating = heatingorsystemtypeid,
  type_aircon = airconditioningtypeid,
  type_architectural= architecturalstyletypeid
)
rename(train_2016)
train_2016 = train_2016 %>% rename(
  id_parcel = parcelid,
  date = transactiondate
)
properties_2016 = properties_2016 %>% 
  mutate(tax_delinquency = ifelse(tax_delinquency=="Y",1,0),
         flag_fireplace = ifelse(flag_fireplace=="Y",1,0),
         flag_tub = ifelse(flag_tub=="Y",1,0))
#```

Now, let's write the data into new files.

#```{r write_newfiles}
write.csv(properties_2016, "renamed_properties_2016.csv")
write.csv(train_2016, "renamed_train_2016.csv")
#```

Let's take a brief look at the data sets. The variables, their type, etc.

```{r echo=FALSE, message=FALSE, warning=FALSE, Summary_of_Data}
str(properties_2016)
str(train_2016)
names(properties_2016)
```

The data set "properties_2016" consists of 58 variables and 2,985,217 observations. The data set "train_2016_v2.csv" contains 3 variables and 90275 observations. 

Let's merge these two data sets, and create a single data set that contains both the property information and the transaction information.

```{r echo=FALSE, message=FALSE, warning=FALSE, merge_datasets}
zillow = merge(train_2016, properties_2016, by = "id_parcel")
str(zillow)
head(zillow)

```

This now will be the main data set that we'll be working with.


# Univariate Plots Section

Our objective is to predict the residual error. So, let's plot a histogram of logerror. I'll plot all histograms in terms of percentages to make a better sense out of them. I'll first do a summary to help me setup the graph.

```{r echo=FALSE, message=FALSE, warning=FALSE, Ditribution_of_logerror }
summary(zillow$logerror)
ggplot(aes(x = logerror, y = ..count../sum(..count..)*100), data = zillow) +
  geom_histogram(fill = 'blue', binwidth = 0.05) +
  xlab("logerror") + ylab("%") +
  coord_cartesian(x=c(-1, 1), y=c(0,45))
```

The x-axis here is the logerror. It actually shows the order of magnitude differences between the estimated values and the sale price. For instance, logerror= 1 indicates that Zestimate overestimated a property value by an order of magnitude, e.g. the sale price was \$100,000 but Zestimate predicted \$1000,000! Let's get a summary of logerror:

```{r echo=FALSE, message=FALSE, warning=FALSE, summary_logerror}
summary(zillow$logerror)
```

Mean and medians are both positive. This indicates that Zestimate on average overestimates the house prices. 

Let's split this over three different counties to see how the distribution looks like.

```{r echo=FALSE, message=FALSE, warning=FALSE, Ditrib_logerror_by_county }

names(table(zillow$region_county))

p1 = ggplot(aes(x = logerror, y = ..count../sum(..count..)*100), data = subset(zillow, region_county == 1286)) +
  geom_histogram(fill = 'blue', binwidth = 0.05) +
  xlab("logerror") + ylab("%") +
  labs(title = "1286") +
  coord_cartesian(x=c(-1, 1), y=c(0,45))

p2 = ggplot(aes(x = logerror, y = ..count../sum(..count..)*100), data = subset(zillow, region_county == 2061)) +
  geom_histogram(fill = 'black', binwidth = 0.05) +
  xlab("logerror") + ylab("%") +
  labs(title = "2061") +
  coord_cartesian(x=c(-1, 1), y=c(0,45))

p3 = ggplot(aes(x = logerror, y = ..count../sum(..count..)*100), data = subset(zillow, region_county == 3101)) +
  geom_histogram(fill = 'red', binwidth = 0.05) +
  xlab("logerror") + ylab("%") +
  labs(title = "3101") +
  coord_cartesian(x=c(-1, 1), y=c(0,45))

grid.arrange(p1, p2, p3, ncol = 3)

```

While the distributions look similar, the code "3101" has longer tails. This could simply be because of higher number of transactions in that county.

I would like to get a overall idea of the size and the age of the buildings.

```{r echo=FALSE, message=FALSE, warning=FALSE, size_hitogram}
s1 = ggplot(aes(x = area_total_calc, y = ..count../sum(..count..)*100), data = subset(zillow, !is.na(area_total_calc))) +
  geom_histogram(fill = "orange", binwidth = 100) +
  xlab("total area") + ylab("%") +
  coord_cartesian(x=c(500, 7500), y=c(0,8)) +
  labs(title = "size linear scale")

s2 = ggplot(aes(x = area_total_calc, y = ..count../sum(..count..)*100), data = subset(zillow, !is.na(area_total_calc))) +
  geom_histogram(fill = "green", binwidth = 0.03) +
  xlab("total area") + ylab("%") +
  coord_cartesian(x=c(500, 7500), y=c(0,8)) +
  scale_x_log10() +
  labs(title = "size log scale")
grid.arrange(s1, s2, ncol = 2)
```

By scaling the x axis to log10, the histogram looks like a normal distribution. 

Let's get a summary of the data as well.

```{r echo=FALSE, message=FALSE, warning=FALSE, size_summary}
summary(zillow$area_total_calc)
```

The average size 1773 sqft and the max size is 2273 sqft. These values make sense. However, min size is 2! This does not make sense. We should be careful with this when we try to fit a model to the data. 

Let's make a similar plot with the tax_total (this is the assessed value of the properties for tax purposes). I would predict to observe a similar behavior.

```{r echo=FALSE, message=FALSE, warning=FALSE, tax_hitogram}
t1 = ggplot(aes(x = tax_total, y = ..count../sum(..count..)*100), data = subset(zillow, !is.na(tax_total))) +
  geom_histogram(fill = "cyan", binwidth = 15000) +
  xlab("total tax") + ylab("%") +
  coord_cartesian(x=c(1e4, 1e7), y=c(0,3)) +
  labs(title = "tax linear scale")

t2 = ggplot(aes(x = tax_total, y = ..count../sum(..count..)*100), data = subset(zillow, !is.na(tax_total))) +
  geom_histogram(fill = "olivedrab", binwidth = 0.02) +
  xlab("total tax") + ylab("%") +
  coord_cartesian(x=c(1e4, 1e7), y=c(0,3)) +
  scale_x_log10() +
  labs(title = "tax log scale")
grid.arrange(t1, t2, ncol = 2)
```


The graphs meet my expectations. However, I see a shoulder in log10 transformed histogram of total_tax which is quite interesting. 


Next, I'll create a new feature called "age". This feature contains the age of each building. I'll then plot a histogram of age.

```{r echo=FALSE, message=FALSE, warning=FALSE, age_hitogram}
zillow$age_year = 2017 - zillow$build_year

ggplot(aes(x = age_year, y = ..count../sum(..count..)*100), data = subset(zillow, !is.na(build_year))) +
  geom_histogram(fill = "lightcoral", binwidth = 5) +
  xlab("age") + ylab("%") +
  coord_cartesian(x=c(0, 120), y=c(0,10)) +
  labs(title = "age") 
```


We don't have too many new buildings in the data set. Let's also get a summary of the data.

```{r echo=FALSE, message=FALSE, warning=FALSE, age_summary}
summary(zillow$age_year)
```

The mean age of the building is ~48 years. The mean and medians are very close here.

# Univariate Analysis

### What is the structure of your dataset?

The main data set contains transactions/logerror data for 90275 properties with 58 features. The features include information such as size, tax, number of rooms, built year as well as x,y coordinates and zone information.  

### What is/are the main feature(s) of interest in your dataset?

The main feature of interest is the logerror which shows the order of magnitude deviation of the predicted price from the sale price.   

### What other features in the dataset do you think will help support your \
investigation into your feature(s) of interest?

I think the coordinate of each property (x and y) and the properties density in a region (this is not included in the dataset) are important features which could be related to the magnitude of logerror. I plan to perform some EDA on the properties coordinates and their correlation with logerro these features. Calculating the feature "density" is quite challenging and suitable for model building later on.

### Did you create any new variables from existing variables in the dataset?

I created a new feature called "age" (the age of each properties expressed in number of years) to better show if a property is newly built or not. The existing feature "year_built" contains this information but I think it's quite easier to comprehend the age rather than the year_built.

### Of the features you investigated, were there any unusual distributions? \
Did you perform any operations on the data to tidy, adjust, or change the form \
of the data? If so, why did you do this?

The distribution of the graphs (logerror, size, age) were not unexpected. The distribution of the size histogram was right skewed. I log-transformed that distribution and the transformed graph looked like a normal distribution. 


# Bivariate Plots Section

I would like to look at the correlation between different features. There are now 61 features in the dataset! We either have to select important features by intuition or find another criterion! Let's do both; first, I'll drop features with more than 50% of missing values.

```{r echo=FALSE, message=FALSE, warning=FALSE, drop_features}
zillow_selected = zillow[, -which(colMeans(is.na(zillow)) > 0.5)]
str(zillow_selected)
```

This leaves us with 35 features. We can now select some features and make correlation plots.

First, let's create the correlation graphs for logerror, num_bedroom, num_bathroom, num_room, and num_units. 

```{r echo=FALSE, message=FALSE, warning=FALSE, correlation_for_numbers}
num_features = zillow_selected[, c("logerror", "num_bedroom", "num_bathroom_calc", "num_room", "num_unit")]
ggpairs(num_features)
```

We see that there is a strong correlation between the number of bathrooms and the number of bedroom in a properties which makes sense. 
The correlation between the logerror and the number of bedroom shows the distribution of the logerror is pretty wide for the properties with 3-5 bedrooms.

Now, let's look at the correlation between the features related to the size and also the age of the buildings.

```{r echo=FALSE, message=FALSE, warning=FALSE, correlation_for_areas_year}
area_year_features = zillow_selected[, c("logerror", "area_total_calc", "area_live_finished", "area_lot", "age_year")]
ggpairs(area_year_features)
```

There is a perfect correlation between the "area_total_calc" and "area_live_finished" with correlation coefficient of 1. These two features should be combined together to create a single feature when making a predictive model.

The correlation plots between the "logerror" and "area_total_calc", "area_live_finished", and "area_lot" shows the logerror is smaller for larger properties. Again, we should keep in mind that the logerror tells us about the order of magnitude of error. 


Finally, let's make a correlation plots for the tax related features.

```{r echo=FALSE, message=FALSE, warning=FALSE, correlation_for_tax}
tax_features = zillow_selected[, c("logerror", "tax_building", "tax_land", "tax_total")]
ggpairs(tax_features)
```

We see that there is not a strong correlation between the tax features and the logerror. However, there is a strong correlation between the tax properties themselves which is not surprising.

Let's explore the month in which the transactions occurred and the correlation with logerror. To do this, I will created a new feature "trans_month" that contains the months information (1, 2, 3, etc). The reasoning behind this comes from the fact that "season change" drives the real estate market. I want to see whether Zestimate takes this into account or not, i.e. is there a strong correlation between the transaction month and the logerror?

```{r echo=FALSE, message=FALSE, warning=FALSE, trans_month}
zillow$trans_month = format(as.Date(zillow$date), "%m")

ggplot(aes(x = trans_month, y = ..count../sum(..count..)*100), data = subset(zillow, !is.na(trans_month))) +
  geom_bar(fill = "brown") +
  xlab("trans_month") + ylab("%") +
  #coord_cartesian(x=c(0, 120), y=c(0,10)) +
  labs(title = "transaction month") 
```

This histogram only tells us about the transaction data available to us. It contains no information about the total number of transactions in each month.

Now, let's plot the average logerror vs transaction month.

```{r echo=FALSE, message=FALSE, warning=FALSE, Conditional Summaries}
trans_month_groups = group_by(zillow, trans_month)
zillow.logerror_by_month = summarise(trans_month_groups,
          logerror_mean = mean(logerror),
          logerror_se = sd(logerror)/sqrt(length(zillow$id_parcel)),
          logerror_median = median(logerror),
          n = n())

ggplot(data = zillow.logerror_by_month, aes(x = trans_month, y = logerror_mean)) +
  geom_point(color = "darkgreen", size = 4)+
  geom_errorbar(color = "darkgreen", size = 1.5, aes(ymin=logerror_mean-2*logerror_se, ymax=logerror_mean+2*logerror_se)) +
  geom_line(color = "darkgreen", size = 1.5, group = 1)

```


Very interesting results. The average logerror decreases as we enter the spring (March, April, etc) and it rises once the summer is finished (September, October, etc). This could very much be a general trend.


# Bivariate Analysis

### Talk about some of the relationships you observed in this part of the \
investigation. How did the feature(s) of interest vary with other features in \
the dataset?

The correlation plots shows that in general logerror is smaller for larger properties. This indicates that Zestimates, in general, predicts the order of magnitude of sale prices for larger properties more accurately. This, however, does mean that the actual dollar amounts are predicted more accurately. For example, when the logerror is 0.1 for a \$100,000 house, the prediction is off by ~$26,000. The deviation from the sale price is ~\$260,000 for a \$1,000,000 house.     

### Did you observe any interesting relationships between the other features \
(not the main feature(s) of interest)?

There is a perfect linear relationship between "area_total_calc" and "area_live_finished". The number of bathroom and bedroom in a property are also strongly correlated. The same is true for the building tax and land tax. 

### What was the strongest relationship you found?

While there hasn't been a very strong correlation between different features and logerror, I found that the logerror is systematically influenced by seasons/months. The logerror is generally lower in spring and summer. 
The overall weak correlation between various features investigated in this section and my feature of interest (logerror), encouraged made me to look into other features (coordinates, etc) in multivariate plot section. It is very important to look at x, y, and logerror in a single plot (rather than in multiple bivariate plots). As such, this task will be completed in the next section.   

# Multivariate Plots Section

First, I would like to create a map and show x and y coordinates of each property using a circle. I also would like to color each circle based on the value of logerror. Here, I'll show a fraction of the data (30%). I'll select them randomly. 

```{r echo=FALSE, message=FALSE, warning=FALSE, map_plot}
zillow.coor_error = zillow %>% 
  transform(longitude = longitude/1e6, latitude = latitude/1e6) %>%
  select(id_parcel, logerror,longitude, latitude) %>% 
  subset(!is.na(longitude) & !is.na(latitude) & abs(logerror) < 0.5 ) %>%
  sample_frac(0.3)

min_lon = min(zillow.coor_error$longitude)
max_lon = max(zillow.coor_error$longitude)
min_lat = min(zillow.coor_error$latitude)
max_lat = max(zillow.coor_error$latitude)

color_bar = colorNumeric('RdYlBu', domain = zillow.coor_error$logerror)
leaflet(zillow.coor_error) %>% addTiles() %>%
  fitBounds(min_lon, min_lat, max_lon, max_lat) %>%
  addCircleMarkers(color=~color_bar(logerror), fillOpacity = 1, stroke = FALSE, radius = 4) %>%
   addLegend("topleft", pal = color_bar, values = ~logerror, title = "logerror", opacity = 1)

```

The yellow color dominates here which indicates, the logerror mainly is very small. However, we can easily spot several blue (overestimated prices) and red (under-estimated prices) on the map. If we zoom in by a factor of 5-6 we'll see quite an interesting behavior. The properties with large values of logerror are commonly more isolated. There are two potential theories on how the predictions are poor for those properties: 1) These properties are unique. For example, they are located in a unique location (good or bad) which can potentially attract or repel buyers. The Zestimate prediction probably doesn't take those factor into the considerations 2) Zestimate takes advantage of the sale prices for other properties in close proximity of a property of interest. Accordingly, Zestimate fails to make an accurate prediction when there isn't enough nearby data.   

In the bivariate section, we saw that the logerror appears to be smaller for larger properties. This was true for both features related to the area (total area, area_live_finished) and tax related features. Let's make a scatter plot of one feature from these two categories and color them based on the logerror values.

```{r echo=FALSE, message=FALSE, warning=FALSE, tax_area_logerror}

p = ggplot(aes(x = tax_total, y = area_total_calc, color = logerror), 
       data = subset(zillow_selected, !is.na(tax_total) & !is.na(area_total_calc) & abs(logerror) < 0.5)) + 
  geom_point(alpha = 0.5, size = 2, aes(colours = logerror)) +
  scale_colour_gradient2() +
  #scale_x_continuous(limits = c(1e4, 1e7)) +
  #scale_y_continuous(limits = c(100, 10000)) +
  scale_x_log10(limits = c(1e4, 2e7)) +
  scale_y_log10(limits = c(100, 10000))
p
```


This plots shows we don't have many extreme logerror for superlarge and super-expensive properties. It also shows we have much less data points there. Therefore, I think "size/assessed tax value" do not heavily influence the logerror (this is also confirmed using the correlation plots). Let's split this by region_county to see if anything changes.

```{r echo=FALSE, message=FALSE, warning=FALSE, tax_area_logerror_by_county}
p + facet_wrap(~region_county)
```

Visually, it looks like we have more outliers in the county "3101"" similar to what we observed before in univariate plots section.

```{r }
p + facet_wrap(~type_quality, drop = TRUE)
```

I don't see any distinct patterns in these plots. 

# Multivariate Analysis

### Talk about some of the relationships you observed in this part of the \
investigation. Were there features that strengthened each other in terms of \
looking at your feature(s) of interest?

The x,y coordinates appear to be quite important in predicting the feature of interest here. It appears very difficult to correctly predict the logerror with high accuracy without an extensive feature creation/manipulation. 

### Were there any interesting or surprising interactions between features?

The map plot showed some interesting interactions with the logerror. It seems that to make we must create some new features that takes the sale price of the nearest properties into account in order to make a good predictive model of logerror . This is supported by observation of large logerror for isolated properties, i.e. the properties with fewer number of neighbors.  

------

# Final Plots and Summary

> **Tip**: You've done a lot of exploration and have built up an understanding
of the structure of and relationships between the variables in your dataset.
Here, you will select three plots from all of your previous exploration to
present here as a summary of some of your most interesting findings. Make sure
that you have refined your selected plots for good titling, axis labels (with
units), and good aesthetic choices (e.g. color, transparency). After each plot,
make sure you justify why you chose each plot by describing what it shows.

### Plot One
```{r echo=FALSE, message=FALSE, warning=FALSE, Plot_One}
p1 = ggplot(aes(x = logerror, y = ..count../sum(..count..)*100), data = subset(zillow, region_county == 1286)) +
  geom_histogram(fill = 'blue', binwidth = 0.05) +
  xlab("logerror") + ylab("percentage of properties") +
  labs(title = "County 1286") +
  coord_cartesian(x=c(-1, 1), y=c(0,45))

p2 = ggplot(aes(x = logerror, y = ..count../sum(..count..)*100), data = subset(zillow, region_county == 2061)) +
  geom_histogram(fill = 'black', binwidth = 0.05) +
  xlab("logerror") + ylab("percentage of properties") +
  labs(title = "County 2061") +
  coord_cartesian(x=c(-1, 1), y=c(0,45))

p3 = ggplot(aes(x = logerror, y = ..count../sum(..count..)*100), data = subset(zillow, region_county == 3101)) +
  geom_histogram(fill = 'red', binwidth = 0.05) +
  xlab("logerror") + ylab("percentage of properties") +
  labs(title = "County 3101") +
  coord_cartesian(x=c(-1, 1), y=c(0,45))

grid.arrange(p1, p2, p3, ncol = 3)
```

### Description One
The distribution of logerror in three different counties appears to be normal. We see a higher number of outliers in county 3101 (longer tails). In general, Zestimates has comparable performance in these three counties of California.  

### Plot Two
```{r echo=FALSE, message=FALSE, warning=FALSE, Plot_Two}
ggplot(data = zillow.logerror_by_month, aes(x = trans_month, y = logerror_mean)) +
  geom_point(color = "darkgreen", size = 4)+
  geom_errorbar(color = "darkgreen", size = 1.5, aes(ymin=logerror_mean-2*logerror_se, ymax=logerror_mean+2*logerror_se)) +
  geom_line(color = "darkgreen", size = 1.5, group = 1) + 
  xlab("transaction month") + ylab("mean of logerror")
```

### Description Two

I find this to be one of the most important plots in this analysis. The plot shows that Zestimate, on average, has the lowest logerrors in spring and summer. I believe the Zestimates' algorithms likely takes the season change into account. However, it's either difficult to predict the market in the fall and winter or their algorithms needs some improvements. 

### Plot Three: The Map Plot

### Description Three

I did not copy that plot here because it significantly slows down the script. I think this graphs contains a lot of information about the data. One can spend hours and hours on this graph and think about possible theories to explain the logerror. I found that Zestimate commonly fails to accurately predict the price of the properties in less dense areas. Accordingly, I hypothesize the Zestimates relies on available sale data to make predictions.

------

# Reflection

> **Tip**: Here's the final step! Reflect on the exploration you performed and
the insights you found. What were some of the struggles that you went through?
What went well? What was surprising? Make sure you include an insight into
future work that could be done with the dataset.

The training data set contains information on ~90,000 properties in three different countries of California. We are provided with 58 different features including the size related information (size of the properties, number of bedrooms, number of bathrooms, etc) as well as tax, and location properties. The ultimate goal is to predict the deviation of the Zillow's model called Zestimate with the actual sale price. I first started my EDA by plotting some basic histograms to get a feel for data. I looked at the histograms of logerror, size, and tax properties and performed some transformation of the data. Then, I made some correlation plots to explore the relationships between important features. I indeed found reasonable correlations between a number of features. However, there was not a strong correlation between different features and my feature of interest logerror. Next, I plotted a conditional summary (mean of logerror for each months), and I found an interesting correlations which showed the logerror is higher is Fall and Winter. I also plotted the x and y coordinates of each property on a map using the leaflet library which also revealed some interesting information. 
One of the difficult task was splitting the histogram of logerror in terms of percentages for different counties. The first solution that came to my mind was using the facet_wrap in ggplot. However, I wanted to calculate the percentage of logerror of each county separately. To accomplish this, I subset the data for each county. 
My future plan is to make a predictive model and participate in the Kaggle competition. The analysis that I performed here and the insights from the data are crucial for making a good model.

#References
The kaggle wesite, stackoverflow, r-bloggers, and other documentations were used in this EDA.